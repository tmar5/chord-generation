{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AutoEncoder.ipynb","provenance":[{"file_id":"1AxNFFZzZ-vKYQBv4SJdz8Ssswtd1kOhv","timestamp":1587314538711}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"SgWKBiqU2e74","colab_type":"code","outputId":"7b31a79d-f5c5-4cb8-f9e2-465c7025d274","executionInfo":{"status":"ok","timestamp":1588005033212,"user_tz":-120,"elapsed":4809,"user":{"displayName":"Julien Hage","photoUrl":"","userId":"02285637608745757761"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PMSPzmdF1fpY","colab_type":"code","colab":{}},"source":["# https://graviraja.github.io/seqtoseqimp/#\n","import os\n","\n","import numpy as np\n","import torch\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from torch import nn\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from tqdm import trange\n","import random\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pm6vrUXA3Y5U","colab_type":"code","colab":{}},"source":["os.chdir(os.path.join(\"drive\", \"My Drive\", \"INF8225\", \"Projet\"))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pe3n7MBwczSt","colab_type":"code","colab":{}},"source":["path_base = '/content/drive/My Drive/INF8225/Projet/' #à modifier selon l'emplacement des données"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JVkz-CQl31ng","colab_type":"code","colab":{}},"source":["os.getcwd()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kwFa6OLs1ZVZ","colab_type":"code","colab":{}},"source":["def data(batch_size):\n","    \"\"\"\n","    DATA from make_npy_padded\n","    Load data when input is : sequences of notes grouped by measures\n","    target: sequence of chords grouped by measures\n","    notes : padded to 32/ measure\n","\n","    Process data for torch\n","    For crossentropy, target must be a number (from 0 to number of classes)\n","\n","    :param batch_size:\n","    :return: train , validation and test dataloader\n","    \"\"\"\n","\n","    train_inputs = np.load(path_base + 'data/train_input_1measures.npy', allow_pickle=True)\n","    train_targets = np.load(path_base+ 'data/train_target_1measures.npy', allow_pickle=True)\n","    test_inputs = np.load(path_base+ 'data/test_input_1measures.npy', allow_pickle=True)\n","    test_targets = np.load(path_base+ 'data/test_target_1measures.npy', allow_pickle=True)\n","\n","\n","    train_inputs = np.concatenate([np.array(train_inputs[i]) for i in range(train_inputs.shape[0])], axis=0)\n","    train_targets = np.concatenate([np.array(train_targets[i]) for i in range(train_targets.shape[0])], axis=0)\n","    test_inputs = np.concatenate([np.array(test_inputs[i]) for i in range(test_inputs.shape[0])], axis=0)\n","    test_targets = np.concatenate([np.array(test_targets[i]) for i in range(test_targets.shape[0])], axis=0)\n","\n","\n","    train_inputs = addStartToken(train_inputs)\n","    test_inputs = addStartToken(test_inputs)\n","\n","    train_inputs, validation_inputs, train_targets, validation_targets = train_test_split(train_inputs, train_targets,\n","                                                                                          random_state=2018,\n","                                                                                          test_size=0.1,\n","                                                                                          shuffle=True)\n","\n","    # Convert all of our data into torch tensors, the required datatype for our modele\n","    train_inputs = torch.tensor(train_inputs, dtype=torch.float)  # float\n","    validation_inputs = torch.tensor(validation_inputs, dtype=torch.float)\n","    train_targets = torch.tensor(train_targets, dtype=torch.float)  # always float!!!\n","    validation_targets = torch.tensor(validation_targets, dtype=torch.float)\n","    test_inputs = torch.tensor(test_inputs, dtype=torch.float)\n","    test_targets = torch.tensor(test_targets, dtype=torch.float)\n","\n","    # Create an iterator of our data with torch DataLoader for memory efficency.\n","    train_data = TensorDataset(train_inputs, train_inputs) #the target is the input for an autoencoder\n","    train_sampler = RandomSampler(train_data)\n","    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","    validation_data = TensorDataset(validation_inputs, validation_inputs) #the target is the input for an autoencoder\n","    validation_sampler = SequentialSampler(validation_data)\n","    validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n","\n","    test_data = TensorDataset(test_inputs, test_inputs) #the target is the input for an autoencoder\n","    test_sampler = SequentialSampler(test_data)\n","    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n","    return train_dataloader, validation_dataloader, test_dataloader"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bLE8TGfGwcaU","colab_type":"code","colab":{}},"source":["def addStartToken(targets, initial_dim=12):\n","  #we add a dimension to code the start token. A note will be a one hot of 13 classes,\n","  # the last one for the start token. We add a start token at the start of each measure.\n","  a,b,c = np.shape(targets)\n","  start_token = [0 for i in range(initial_dim)] + [1]\n","  start_token_array = np.array([[start_token]for i in range(a)])\n","  dim_supp = np.array([[[0] for i in range(b)] for j in range(a)])\n","  targets_dim_supp = np.concatenate((targets,dim_supp), axis=2)\n","  targets_with_start = np.concatenate((start_token_array, targets_dim_supp), axis=1)\n","  return targets_with_start"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1M-U9CpS_9Rs","colab_type":"code","colab":{}},"source":["def accuracySansZeros(outputs, targets, targets_maxvalue):\n","    \"\"\"\n","    :param outputs: (batch_size, 32, 13) from model where 13 sized vector is probas (logits) for one note, the 13th element is for the start token\n","    :param targets: (batch_size) , each chord is a number meaning its class\n","    :param targets_maxvalue: (batch_size), used to know if the target vector was a padded vector of zeros\n","    :return: accuracy for one batch, where padded vector of zeros are not inculuded\n","    \"\"\"\n","    tot_prediction = targets.shape[0]\n","    # log croissant donc log:softmax => valeur désirée est la plus grande (plus grande proba)\n","    idx_predicted = np.argmax(outputs, 1)\n","    values_to_ignore = (targets_maxvalue == 0).sum()\n","    return ((idx_predicted == targets).sum() - values_to_ignore) / (tot_prediction - values_to_ignore)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4qrZ-bzH1pJO","colab_type":"code","colab":{}},"source":["class Encodeur(nn.Module):\n","    \"\"\" Sequence to sequence networks consists of Encoder and Decoder modules.\n","    This class contains the implementation of Encoder module.\n","    Args:\n","        input_dim: A integer indicating the size of input dimension.\n","        hidden_dim: A integer indicating the hidden dimension of RNN layers.\n","        n_layers: A integer indicating the number of layers.\n","        dropout: A float indicating dropout.\n","\n","    if bidirectionnal, we need to manually concatenate the final states and cells for both directions\n","    => for each layer, SUM hidden state on backward and forward direction (and cell state)\n","    \"\"\"\n","\n","    def __init__(self, input_dim=32*13, hidden_dim=256, output_dim = 50, n_layers=2, dropout=0.3, bidirectional=True):\n","        super().__init__()\n","        self.input_dim = input_dim\n","        self.hidden_dim = hidden_dim\n","        self.n_layers = n_layers\n","        self.dropout = dropout\n","        self.bidirectional = bidirectional\n","\n","        self.lstm = nn.LSTM(input_size=input_dim,\n","                            hidden_size=hidden_dim,\n","                            num_layers=n_layers,\n","                            dropout=dropout,\n","                            bidirectional=bidirectional,\n","                            # ini decodeur avec toujours le mm last cell state and hiddenstate?\n","                            batch_first=True\n","                            )\n","        self.linear = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        return for last layer of lstm (on the top):\n","            - hidden state for all its cells (seq_len hidden states)\n","            - hidden cell state for all its cells\n","        We will need only the h_s et c_s of the last cell of the last layer, for decoder initialization\n","\n","\n","        hidden state if bidirectional: [forward_1, backward_1, forward_2, backward_2, ...]\n","        = for each layer for each direction the last hidden state\n","        => we need to sum / match dimension (two context vector for each layer, decoder need only one) for bw/fw\n","\n","        :param x: [batch_size, sentence_length, input_size]\n","        :return:\n","        \"\"\"\n","        #we flatten the measure matrix in order to have a big vector to encode\n","        x = x.view(x.shape[0],1,x.shape[1]*x.shape[2])\n","\n","        outputs, (hidden, cell) = self.lstm(x)  # last hidden /cell (for each layer, and for each direction)\n","        \n","        outputs = self.linear(outputs.squeeze(1))\n","        outputs = F.log_softmax(outputs)\n","        \n","        if self.bidirectional:\n","            # Sum backward and forward (add first dim as 0 with squeeze for concatenation of the two layers)\n","            hid_layer1 = (hidden[0, :, :] + hidden[1, :, :]).unsqueeze(0)\n","            hid_layer2 = (hidden[2, :, :] + hidden[3, :, ]).unsqueeze(0)\n","            hidden = torch.cat((hid_layer1, hid_layer2), dim=0)\n","\n","            cell_layer1 = (cell[0, :, :] + cell[1, :, :]).unsqueeze(0)\n","            cell_layer2 = (cell[2, :, :] + cell[3, :, ]).unsqueeze(0)\n","            cell = torch.cat((cell_layer1, cell_layer2), dim=0)\n","\n","        return outputs, hidden, cell"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"11560vyU1sjk","colab_type":"code","colab":{}},"source":["class Decoder(nn.Module):\n","    \"\"\" This class contains the implementation of Decoder Module.\n","    Args:\n","        output_dim: A integer indicating the size of output dimension.\n","        hidden_dim: A integer indicating the hidden size of rnn.\n","        n_layers: A integer indicating the number of layers in rnn.\n","        dropout: A float indicating the dropout.\n","        input_dim: input dimension for the decodeur (chord one hot len)\n","\n","    \"\"\"\n","\n","    def __init__(self, input_dim=13, output_dim=13, hidden_dim=256, n_layers=2, dropout=0.3):\n","        super().__init__()\n","\n","        self.output_dim = output_dim\n","        self.hidden_dim = hidden_dim\n","        self.n_layers = n_layers\n","\n","        self.lstm = nn.LSTM(input_size=input_dim,\n","                           hidden_size=hidden_dim,\n","                           num_layers=n_layers,\n","                           dropout=dropout,\n","                           batch_first=True, \n","                           )\n","        self.linear = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, input, hidden, cell):\n","        # input is of shape [batch_size, note_len]\n","        # = the i-th (note of the sequence of target/previous decoder output) for each sample\n","        # hidden is of shape [batch_size, n_layer * num_directions, hidden_size]\n","        # cell is of shape [batch_size, n_layer * num_directions, hidden_size]\n","\n","\n","        input = input.unsqueeze(1)  # unsqueeze() inserts singleton dim at position given as parameter\n","        # input shape is [batch_size, output_dim]. reshape is needed rnn expects a rank 3 tensors as input.\n","        # so reshaping to [batch_size, 1, output_dim] means a batch of batch_size each containing 1 index.\n","\n","\n","        output, (hidden, cell) = self.lstm(input, (hidden, cell))\n","\n","        predicted = self.linear(output.squeeze(1))  # linear expects as rank 2 tensor as input, remove\n","        # predicted shape is [batch_size, output_dim]\n","        predicted = F.log_softmax(predicted)  # one hot prediction\n","\n","        return predicted, hidden, cell"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5tqTDlEH1vKS","colab_type":"code","colab":{}},"source":["class Seq2Seq(nn.Module):  # TODO add attention\n","    \"\"\" This class contains the implementation of complete autoencoder network.\n","    It uses to encoder to produce the context vectors.\n","    It uses the decoder to produce the predicted target sentence.\n","    Args:\n","        encoder: A Encoder class instance.\n","        decoder: A Decoder class instance.\n","\n","    Decoder : input should begin with <start> and target ends with <end> (otherwise learns identity)\n","    \"\"\"\n","\n","    def __init__(self, encoder, decoder):\n","        super().__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","\n","    def forward(self, inputs, targets, teacher_forcing_ratio=0.3):\n","        # we don't have embedding layer in decoder/encoder\n","        #   - inputs (sequence of notes) is size [batch_size, seq_len, note_len]. Others size diff also\n","        #   - targets is of shape [batch_size, sequence_len, note_len]\n","        # if teacher_forcing_ratio is 0.3 we use ground-truth inputs 30% of time and 70% time we use decoder outputs.\n","\n","        batch_size = targets.shape[0]\n","        max_len = targets.shape[1]  # len of the sequence of notes outputed by decoder       \n","        target_size = targets.shape[2]  # = self.decoder.output_dim\n","\n","        # to store the outputs of the decoder\n","        outputs = torch.zeros(batch_size, max_len, target_size)\n","\n","        # context vector, last hidden and cell state of encoder to initialize the decoder (from each layer)\n","        _, hidden, cell = self.encoder(inputs[:,1:,:]) #without the start token\n","\n","        # first input to the decoder is the <sos> tokens # the first note of the seq for each sample of the batch\n","        input = targets[:, 0] # size [batch_size, chord_length]\n","        for t in range(1, max_len):\n","            # pass the input, previous hidden and previous cell states into the decoder (forward)\n","            # receive a prediction, next hidden state and next cell state from the decoder\n","            output, hidden, cell = self.decoder(input, hidden, cell)\n","            # place our prediction / output in our tensor of predictions (output= [batch_size, note_size])\n","            outputs[:, t, :] = output\n","            # decide if we are going to “teacher force” or not\n","            use_teacher_force = random.random() < teacher_forcing_ratio\n","            # target: batch_size*note_size => 1 note target for each sample\n","            input = (targets[:, t, :] if use_teacher_force else output)\n","\n","              # outputs is of shape [batch_size, sequence_len, output_dim]\n","        \n","        return outputs"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mh_geqQF1y_4","colab_type":"code","colab":{}},"source":["def train(model, batch_size=1024, learning_rate=0.001, eps=1e-8, epochs=10, patience=15, model_name=\"Normal\"):\n","    \"\"\"\n","    target: must be a single number, not a one hot... for using cross_entropy\n","    :param patience: break if no improvment of validation accuracy after waiting 10 epochs\n","    :return:\n","    \"\"\"\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    best_loss = 100\n","    best_model = None\n","\n","    train_dataloader, validation_dataloader, _ = data(batch_size)\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, eps=eps)\n","    # PyTorch scheduler: reduce lr by factor after patience with no improvment of val_loss (mieux que StepLR)\n","    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=False, threshold=0.0001,\n","                                  threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08)\n","    criterion = nn.NLLLoss()  # si nn.logsoftmax dans le modele (pour le test)\n","\n","    max_grad_norm = 2.0  # Gradient threshold, gradients norms that exceed this threshold are scaled down to match\n","    # the norm.\n","\n","    # Tracking values\n","    val_accuracies = list()\n","    val_losses = list()\n","    train_accuracies = list()\n","    train_losses = list()\n","\n","    p = 0  # patience\n","\n","    model.cuda()\n","    for epoch in trange(epochs, desc=\"Epoch\"):\n","        if p >= patience:\n","            break\n","\n","        model.zero_grad()  # Clear stored gradient\n","        model.train()  # training mode\n","\n","        # Tracking variables\n","        tr_loss, tr_acc, nb_tr_steps = 0, 0, 0\n","\n","        # Train on all our data for one epoch\n","        for step, batch in enumerate(train_dataloader):\n","            # Add batch to GPU\n","            b_inputs, b_targets = tuple(t.to(device) for t in batch)\n","\n","\n","            # Forward pass\n","            # output is of shape [batch_size, sequence_len, output_dim] = 1024*4*24\n","            outputs = model(b_inputs, b_targets)\n","\n","            outputs = outputs[:, 1:, :]\n","            b_targets = b_targets[:, 1:, :]  # ignore start token\n","\n","            # loss function (negative log likelihood because of the last log softmax layer) = multiclass cross-entropy\n","            # \"size_average\" is set to \"False\", the losses are summed for each minibatch.\n","            # average loss for all predicted chords of the seq\n","            # NLL loss need index and not onehot. Take argmax of one hot chord (axe = 2)\n","            b_targets_maxvalues, b_targets = b_targets.max(2)  # max return (values of max, indices)\n","            b_targets_maxvalues = b_targets_maxvalues.view(-1).to(device)\n","            b_targets = b_targets.view(-1).to(device)\n","            outputs = outputs.reshape(outputs.shape[0]*outputs.shape[1], outputs.shape[2]).to(device)\n","            loss = criterion(outputs, b_targets)  # -1: torch chose. view: reshape tensor\n","            # Backward pass\n","            loss.backward()\n","\n","            # Update tracking variables\n","            tr_loss += loss.item()\n","\n","            # Move logits and labels to CPU\n","            outputs = outputs.detach().cpu().numpy()\n","            b_targets = b_targets.to('cpu').numpy()\n","            tr_acc += accuracySansZeros(outputs, b_targets, b_targets_maxvalues)\n","\n","            # AdamW optimizer: update parameters and take a step using the computed gradient\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n","            optimizer.step()  # Perform a step of gradient descent\n","            model.zero_grad()  # Reset gradient\n","\n","            nb_tr_steps += 1\n","\n","        train_accuracy = tr_acc / nb_tr_steps\n","        train_loss = tr_loss / nb_tr_steps\n","        train_accuracies.append(train_accuracy)\n","        train_losses.append(train_loss)\n","\n","\n","        # 2. Validation step\n","        # Put model in evaluation mode to evaluate loss on the validation set\n","        model.eval()\n","\n","        # Tracking variables\n","        nb_eval_steps, v_acc, v_loss = 0, 0, 0\n","\n","        # Evaluate data for one epoch\n","        for batch in validation_dataloader:\n","            # Add batch to GPU, Unpack the inputs from our dataloader\n","            b_inputs, b_targets = tuple(t.to(device) for t in batch)\n","            # Telling the model not to compute or store gradients, saving memory and speeding up validation\n","            with torch.no_grad():\n","                # Forward pass, calculate logit predictions\n","                # turn off the teacher forcing\n","                outputs = model(b_inputs, b_targets, 0)\n","\n","            outputs = outputs[:, 1:, :]\n","            b_targets = b_targets[:, 1:, :]  # ignore start token\n","\n","            # # Change shape for computing mean accuracy over sequence / mean loss\n","            b_targets_maxvalues, b_targets = b_targets.max(2)  # max return (values of max, indices)\n","            b_targets_maxvalues = b_targets_maxvalues.view(-1).to(device)\n","            b_targets = b_targets.view(-1).to(device)\n","            outputs = outputs.reshape(outputs.shape[0]*outputs.shape[1], outputs.shape[2]).to(device)\n","\n","            v_loss += criterion(outputs, b_targets).item()\n","\n","            # Move logits and labels to CPU\n","            outputs = outputs.detach().cpu().numpy()\n","            b_targets = b_targets.to('cpu').numpy()\n","            v_acc += accuracySansZeros(outputs, b_targets,b_targets_maxvalues)\n","\n","            nb_eval_steps += 1\n","        val_accuracy = v_acc / nb_eval_steps\n","        val_loss = v_loss/nb_eval_steps\n","        val_accuracies.append(val_accuracy)\n","        val_losses.append(val_loss)\n","\n","        # avoid overfitting\n","        if val_loss < best_loss:\n","            p = 0\n","            best_model = model\n","            best_encoder = model.encoder\n","            best_loss = val_loss\n","            torch.save(best_model.state_dict(), path_base+ 'model_torch/Autoencoder' + model_name)\n","            torch.save(best_encoder.state_dict(), path_base+ 'model_torch/Encoder' + model_name)\n","        else:\n","            p += 1\n","\n","        print(\"Train loss: {} - Train accuracy: {} -  Val loss: {} - Val accuracy: {}\".format(train_loss, train_accuracy\n","                                                                                              , val_loss, val_accuracy))\n","\n","        scheduler.step(val_loss)\n","\n","    return best_model, train_losses, train_accuracies, val_accuracies, val_losses\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8eKDyzBL12Pv","colab_type":"code","colab":{}},"source":["def test(model, batch_size=256):  # TODO sortir ouput pour une chanson avec index chanson\n","    \"\"\"\n","    Test model on test songs\n","    :param mode: type of vector/feature to keep\n","    :param batch_size:\n","    :param model:\n","    :return:\n","    \"\"\"\n","    _, _, test_dataloader = data(batch_size)\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.to(device)\n","    model.eval()\n","    test_acc = 0\n","    nb_tr_steps = 0\n","    with torch.no_grad():\n","        for step, batch in enumerate(test_dataloader):\n","            # Add batch to GPU\n","            b_inputs, b_targets = tuple(t.to(device) for t in batch)\n","            # No teacher forcing ############# but target[0] is used as first input decoder !! should be <start>\n","            \n","            outputs = model(b_inputs, b_targets, 0)\n","\n","\n","            outputs = outputs[:, 1:, :]\n","            b_targets = b_targets[:, 1:, :]  # ignore start token\n","\n","            # # # Change shape for computing mean accuracy over sequence\n","            b_targets_maxvalues, b_targets = b_targets.max(2)  # max return (values of max, indices)\n","            b_targets_maxvalues = b_targets_maxvalues.view(-1).to(device)\n","            b_targets = b_targets.view(-1).to(device)\n","            outputs = outputs.reshape(outputs.shape[0]*outputs.shape[1], outputs.shape[2]).to(device)\n","\n","            outputs = outputs.detach().cpu().numpy()\n","            b_targets = b_targets.to('cpu').numpy()\n","\n","            test_acc += accuracySansZeros(outputs, b_targets, b_targets_maxvalues)\n","            nb_tr_steps += 1\n","        test_accuracy = test_acc / nb_tr_steps\n","    print(\"Test accuracy: {}\".format(test_accuracy))\n","    pass"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TogiwFa1o7m4","colab_type":"code","colab":{}},"source":["def show_result(train_losses, train_accuracies, val_accuracies, val_losses):\n","    plt.plot(np.arange(len(val_accuracies)), train_losses, label='train loss')\n","    plt.plot(np.arange(len(val_accuracies)), val_losses, label='val loss')\n","    plt.legend(loc=\"upper right\")\n","    plt.title(\"Negative log-likelihood val-train\")\n","    plt.xlabel(\"Epochs\")\n","    plt.ylabel(\"Loss\")\n","    plt.savefig(path_base+ 'loss1.png')\n","    plt.show()\n","\n","    plt.plot(np.arange(len(val_accuracies)), train_accuracies, label='train accuracy')\n","    plt.plot(np.arange(len(val_accuracies)), val_accuracies, label='val accuracy')\n","    plt.legend()\n","    plt.title(\"Accuracy over validation/train-set\")\n","    plt.xlabel(\"Epochs\")\n","    plt.ylabel(\"Accuracy\")\n","    plt.savefig(path_base+ 'accuracy1.png')\n","    plt.show()\n","    pass"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hWRxzkN314MW","colab_type":"code","colab":{}},"source":["  #train\n","  encoder = Encodeur(input_dim=32*13, bidirectional=False)\n","  decoder = Decoder()\n","  model = Seq2Seq(encoder, decoder)\n","  best_model, train_losses, train_accuracies, val_accuracies, val_losses = train(model, model_name=\".pth\", batch_size=256, learning_rate=0.001, epochs = 100)\n","  print(\"DONE\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HYa7wUQB7E8T","colab_type":"code","colab":{}},"source":["  #test\n","  encoder = Encodeur(input_dim=32*13, bidirectional=False)\n","  decoder = Decoder()\n","  model_trained = Seq2Seq(encoder, decoder)\n","  model_trained.load_state_dict(torch.\n","  load(\"/content/drive/My Drive/INF8225/Projet/model_torch/Autoencoder.pth\"))\n","  test(model_trained, batch_size=128)\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MP7oFxYd7TxR","colab_type":"code","colab":{}},"source":["#plot results\n","show_result(train_losses, train_accuracies, val_accuracies, val_losses)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KvC8sD8Xnkr1","colab_type":"code","colab":{}},"source":["def encode_dataset(encoder, inputs_by_songs, targets_by_songs, random_sampler=False, loader_batch_size=1024, nb_measure=4):\n","  \"\"\"\n","    :param encoder: the trained encoder which will encode our inputs\n","    :param inputs_by_songs: inputs grouped by songs in order not to group 4 measures from two different songs\n","    :param targets_by_songs: targets_grouped_by_songs\n","    :praram random_sampler: boolean deciding if the sampler to use to load build the dataloader is random (for the training set) or sequential(for the validation and test sets)\n","    :param loader_batch_size\n","    :param nb_measure: the size of the grouped measures after encoding the input in order to feed the Seq2Seq model\n","    :return: a dataloader the can be loaded by the Seq2Seq model\n","    \"\"\"\n","  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","  encoder.cuda()\n","  grouped_measures = list()\n","  grouped_chords = list()\n","  for i in range(inputs_by_songs.shape[0]):\n","    inputs = addStartToken(inputs_by_songs[i])\n","    inputs = torch.tensor(inputs, dtype=torch.float)\n","    targets = torch.tensor(targets_by_songs[i], dtype=torch.float)\n","    targets = targets.view(targets.shape[0], targets.shape[2])\n","    data = TensorDataset(inputs, targets)\n","    sampler = SequentialSampler(data)\n","    dataloader = DataLoader(data, sampler=sampler, batch_size=inputs.shape[0])\n","    with torch.no_grad():\n","      for step, batch in enumerate(dataloader):\n","        b_inputs, b_targets = tuple(t.to(device) for t in batch)\n","        encoded_song, _, _ = encoder(b_inputs[:,1:,:])\n","        grouped_measures = grouped_measures + [encoded_song[x:x + nb_measure] for x in range(0,encoded_song.shape[0]-nb_measure)]\n","        grouped_chords = grouped_chords + [b_targets[x:x+nb_measure] for x in range(0,b_targets.shape[0]-nb_measure)]\n","  encoded_inputs = torch.stack(grouped_measures)\n","  targets = torch.stack(grouped_chords)\n","  data = TensorDataset(encoded_inputs, targets)\n","  if random_sampler:\n","    sampler = RandomSampler(data)\n","  else:\n","    sampler = SequentialSampler(data)\n","  dataloader = DataLoader(data, sampler=sampler, batch_size=loader_batch_size)\n","  return dataloader"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kHMQU4Ur-5n6","colab_type":"code","colab":{}},"source":["#Encoding inputs in order to save them and use them in our Seq2Seq architecture. Unfortunately, we weren't able to make our model work with these encoded inputs\n","\n","encoder = Encodeur(input_dim=32*13, bidirectional=False)\n","encoder.load_state_dict(torch.\n","  load(path_base+ 'model_torch/Encoder.pth'))\n","\n","\n","#data comes from make_npy_groups_measure, with n_measures=1. Measures are grouped by songs\n","\n","train_inputs_by_songs = np.load(path_base+ 'data/train_input_1measures.npy', allow_pickle=True)\n","train_targets_by_songs = np.load(path_base+ 'data/train_target_1measures.npy', allow_pickle=True)\n","test_inputs_by_songs = np.load(path_base+ 'data/test_input_1measures.npy', allow_pickle=True)\n","test_targets_by_songs = np.load(path_base+ 'data/test_target_1measures.npy', allow_pickle=True)\n","\n","train_inputs_by_songs, validation_inputs_by_songs, train_targets_by_songs, validation_targets_by_songs = train_test_split(train_inputs_by_songs, train_targets_by_songs,\n","                                                                                                                          random_state=2018,\n","                                                                                                                          test_size=0.1,\n","                                                                                                                          shuffle=True)\n","\n","encoded_train_dataloader = encode_dataset(encoder,train_inputs_by_songs, train_targets_by_songs, random_sampler=True)\n","encoded_validation_dataloader = encode_dataset(encoder,validation_inputs_by_songs, validation_targets_by_songs)\n","encoded_test_dataloader = encode_dataset(encoder,test_inputs_by_songs, test_targets_by_songs, loader_batch_size=256)\n","\n","torch.save(encoded_train_dataloader, path_base+ 'data/encoded_train_dataloader.pth')\n","torch.save(encoded_validation_dataloader, path_base+ 'data/encoded_validation_dataloader.pth')\n","torch.save(encoded_test_dataloader, path_base+ 'data/encoded_test_dataloader.pth')"],"execution_count":0,"outputs":[]}]}