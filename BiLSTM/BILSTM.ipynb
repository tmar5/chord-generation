{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BILSTM.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP+AIhKJJtKNd8yPHOumPm+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"pn_cbHF3I5wC","colab_type":"text"},"source":["This notebook presents a BiLSTM model to generate chords giving a leadsheet.\n"]},{"cell_type":"code","metadata":{"id":"8kFRpPZ9erde","colab_type":"code","colab":{}},"source":["import os\n","import numpy as np\n","import torch\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from torch import nn\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from tqdm import trange"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QkpAbgCMePuN","colab_type":"code","outputId":"fb6474ed-5f17-48ce-c7a2-dcb71e8ffc3e","executionInfo":{"status":"ok","timestamp":1588035513198,"user_tz":240,"elapsed":437,"user":{"displayName":"M G","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiCpAKw7tbqA0zqKy0wtu5Q9vlxLL8FZofHR2NDseg=s64","userId":"10818001129093414844"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mshMONwJeI9V","colab_type":"code","colab":{}},"source":["# Change to the adequate directory\n","os.chdir(os.path.join(\"drive\", \"My Drive\", \"Projet_rendu\", \"BiLSTM\"))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MChPGdBQem0d","colab_type":"code","colab":{}},"source":["def data(batch_size):\n","    \"\"\"\n","    New format of input and target vectors\n","\n","    :param batch_size:\n","    :return:\n","    Process data for torch\n","    For crossentropy, target must be a number (from 0 to number of classes)\n","\n","\n","    measures : padded to 184/song (max measure of in a song in dataset)\n","    notes : padded to 32/ measure\n","        => Suppression des mesures rajoutées au padding (vides). Les mesures paddées ont un chord de 0 correspondant\n","    :param batch_size:\n","    :return: train , validation and test dataloader\n","    \"\"\"\n","    # Loading our data (already padded)\n","    # nbr_song * nbr_measures_kept_by_song * nbr_notes_by_measure * onehot_note\n","    train_inputs = np.load('Données/train_input.npy', allow_pickle=True)\n","    # nbr_songs * nbr_measures_kept * onehot_chord\n","    train_targets = np.load('Données/train_target.npy', allow_pickle=True)\n","    test_inputs = np.load('Données/test_input.npy', allow_pickle=True)\n","    test_targets = np.load('Données/test_target.npy', allow_pickle=True)\n","\n","    train_targets = np.array([np.argmax(i) for i in train_targets])  # index => for crossentropy loss\n","    test_targets = np.array([np.argmax(i) for i in test_targets])\n","\n","    train_inputs, validation_inputs, train_targets, validation_targets = train_test_split(train_inputs, train_targets,\n","                                                                                          random_state=2018,\n","                                                                                          test_size=0.1,\n","                                                                                          shuffle=True)\n","\n","    # Convert all of our data into torch tensors, the required datatype for our modele\n","    train_inputs = torch.tensor(train_inputs, dtype=torch.float)  # float\n","    validation_inputs = torch.tensor(validation_inputs, dtype=torch.float)\n","    train_targets = torch.tensor(train_targets, dtype=torch.long)  # long: classification\n","    validation_targets = torch.tensor(validation_targets, dtype=torch.long)\n","    test_inputs = torch.tensor(test_inputs, dtype=torch.float)\n","    test_targets = torch.tensor(test_targets, dtype=torch.long)\n","\n","    # Create an iterator of our data with torch DataLoader for memory efficency.\n","    train_data = TensorDataset(train_inputs, train_targets)\n","    train_sampler = RandomSampler(train_data)\n","    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","    validation_data = TensorDataset(validation_inputs, validation_targets)\n","    validation_sampler = SequentialSampler(validation_data)\n","    validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n","\n","    test_data = TensorDataset(test_inputs, test_targets)\n","    test_sampler = SequentialSampler(test_data)\n","    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n","\n","    return train_dataloader, validation_dataloader, test_dataloader  # TODO num workers CPU + pin_memory = True\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0LoS3MZLenJc","colab_type":"code","colab":{}},"source":["class BiLSTM(nn.Module):\n","    \"\"\"\n","    input => 1 measure/bar = sequence of 32 notes (onehot size 12)\n","    output: chord, single value (=index in onehot chord vector sized 24 in target)\n","\n","    Achieve 0.51 accuracy on validation set, 0.463 on test\n","    # TODO: masking padding values, batch normalization, confusion matrix, testset, patience=10, other lstm parameters...\n","    \"\"\"\n","\n","    def __init__(self, dropout=0.3, hidden_size=256, num_layer=2, input_size=12, bidirectional=True):\n","        \"\"\"\n","\n","        :param dropout: dropout rate between each of the layer (2) of the LSTM.\n","        only our hidden state from the first layer is passed as input to the second layer, and not the cell state\n","        :param hidden_size:\n","        :param num_layer:\n","        :param input_size: note_dim, 12 , sequence de 32 vecteurs.\n","        Le lstm retourne un seul input (pas besoin d'un input par entrée de la séquence), de taille hidden_size\n","        \"\"\"\n","\n","        super().__init__()\n","        self.bidirectional=bidirectional\n","        self.lstm = nn.LSTM(input_size=input_size,\n","                            hidden_size=hidden_size,\n","                            num_layers=num_layer,\n","                            dropout=dropout,\n","                            bidirectional=bidirectional,\n","                            batch_first=True)\n","        self.fc = nn.Linear(in_features=2 * hidden_size, out_features=24)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        entier sequence at once, only one measure as input so one chord at output, no\n","        need to generate one chord every measure like in a sequence of measure\n","\n","        hidden state and cell state: in torch by default initialized to 0\n","        lstm: every hidden state produce output. Keep last hidden state only\n","        Bidirectional: hiddenstate dim *2\n","\n","        Keep only output from last hidden state of the LSTM that correspond to the representation of the entire sequence\n","        (each output is size 256=2*hidden_size, 32 output because sequence of length 32)\n","        When using a BiLSTM the hidden states of the directions are just concatenated\n","        :param x:\n","        :return:\n","        \"\"\"\n","        outputs, (hidden, cell) = self.lstm(x)  # (batch_size, seq_len, 2*hidden_size)\n","        output = outputs[:, -1, :]  # (1024, 256). -1: pick the last hidden state (last element of the seq)\n","\n","        output = self.fc(output)  # (batch_size, 24)\n","        output = F.log_softmax(output)  # si loss pas crossentropy, sinon inutile\n","        return output  # sans F.logsoftmax si crossentropy car logsoftmax computed directly in crossentropy loss"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mdFOdsQXenSC","colab_type":"code","colab":{}},"source":["def accuracy(outputs, targets):\n","    \"\"\"\n","    :param outputs: (batch_size, 24) from model where 24 sized vector is probas (logits) for one chord\n","    :param targets: (batch_size) , each chord is a number meaning its class\n","    :return: accuracy for one batch\n","    # TODO use torch.max(tensor, 1 ou 0) pour indice et value tjrs dans le gpu\n","    \"\"\"\n","    batch_size = targets.shape[0]\n","    # log croissant donc log:softmax => valeur désirée est la plus grande (plus grande proba)\n","    idx_predicted = np.argmax(outputs, 1)\n","    return (idx_predicted == targets).sum() / batch_size"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CeABzKOLenQJ","colab_type":"code","colab":{}},"source":["def train(model, train_dataloader, validation_dataloader, learning_rate=0.001, eps=1e-8, epochs=400, patience=15,\n","          model_name=\"BILSTM.pth\"):\n","    \"\"\"\n","    target: must be a single number, not a one hot... for using cross_entropy\n","    :param patience: break if no improvment of validation accuracy after waiting s10 epochs\n","    :param eps:\n","    :param model:\n","    :param batch_size:\n","    :param learning_rate:\n","    :param epochs:\n","    :return:\n","    \"\"\"\n","    # device_name = tf.test.gpu_device_name()\n","    # if device_name != '/device:GPU:0':\n","    #     raise SystemError('GPU device not found')\n","    # print('Found GPU at: {}'.format(device_name))\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    best_loss = 100\n","    best_model = None\n","\n","    num_training_steps = (len(train_dataloader) * epochs)  # len(train_dataloader) = len(train_data)// batch_size\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, eps=eps)\n","    # PyTorch scheduler: reduce lr by factor after patience with no improvment of val_loss (mieux que StepLR)\n","    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=False, threshold=0.0001,\n","                                  threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08)\n","    # criterion = nn.CrossEntropyLoss()  # Because classification pb (if regression, MSE). = nn.logsoftmax + nn.NLLLoss\n","    criterion = nn.NLLLoss()  # si nn.logsoftmax dans le modele (pour le test)\n","\n","    max_grad_norm = 1.0  # Gradient threshold, gradients norms that exceed this threshold are scaled down to match\n","    # the norm.\n","\n","    # Tracking values\n","    val_accuracies = list()\n","    val_losses = list()\n","    train_accuracies = list()\n","    train_losses = list()\n","\n","    p = 0  # patience\n","\n","    model.cuda()\n","    for epoch in trange(epochs, desc=\"Epoch\"):\n","        if p >= patience:\n","            break\n","\n","        model.zero_grad()  # Clear stored gradient\n","        model.train()  # training mode\n","\n","        # Tracking variables\n","        tr_loss, tr_acc, nb_tr_steps = 0, 0, 0\n","\n","        # Train on all our data for one epoch\n","        for step, batch in enumerate(train_dataloader):\n","            # Add batch to GPU\n","            b_inputs, b_targets = tuple(t.to(device) for t in batch)\n","\n","            # Forward pass\n","            outputs = model(b_inputs)\n","\n","            # loss function (negative log likelihood because of the last log softmax layer) = multiclass cross-entropy\n","            # \"size_average\" is set to \"False\", the losses are summed for each minibatch.\n","            loss = criterion(outputs, b_targets)\n","\n","            # Backward pass\n","            loss.backward()\n","\n","            # Update tracking variables\n","            tr_loss += loss.item()\n","\n","            # Move logits and labels to CPU\n","            outputs = outputs.detach().cpu().numpy()\n","            b_targets = b_targets.to('cpu').numpy()\n","            tr_acc += accuracy(outputs, b_targets)\n","\n","            # AdamW optimizer: update parameters and take a step using the computed gradient\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n","            optimizer.step()  # Perform a step of gradient descent\n","            model.zero_grad()  # Reset gradient\n","\n","            nb_tr_steps += 1\n","            # if nb_tr_steps % 50 == 0:\n","            #     print(\"Step {} out  of {} /n\".format(nb_tr_steps, (num_training_steps // epochs)))\n","\n","        train_accuracy = tr_acc / nb_tr_steps\n","        train_loss = tr_loss / nb_tr_steps\n","        train_accuracies.append(train_accuracy)\n","        train_losses.append(train_loss)\n","\n","        # 2. Validation step\n","        # Put model in evaluation mode to evaluate loss on the validation set\n","        model.eval()\n","\n","        # Tracking variables\n","        nb_eval_steps, v_acc, v_loss = 0, 0, 0\n","\n","        # Evaluate data for one epoch\n","        for batch in validation_dataloader:\n","            # Add batch to GPU, Unpack the inputs from our dataloader\n","            b_inputs, b_targets = tuple(t.to(device) for t in batch)\n","            # Telling the model not to compute or store gradients, saving memory and speeding up validation\n","            with torch.no_grad():\n","                # Forward pass, calculate logit predictions\n","                outputs = model(b_inputs)\n","\n","            v_loss += criterion(outputs, b_targets).item()\n","\n","            # Move logits and labels to CPU\n","            outputs = outputs.detach().cpu().numpy()\n","            b_targets = b_targets.to('cpu').numpy()\n","\n","            v_acc += accuracy(outputs, b_targets)\n","            nb_eval_steps += 1\n","        val_accuracy = v_acc / nb_eval_steps\n","        val_accuracies.append(val_accuracy)\n","\n","        val_loss = v_loss / nb_eval_steps\n","        val_losses.append(val_loss)\n","\n","        # avoid overfitting\n","        if val_loss < best_loss:\n","            p = 0\n","            best_model = model\n","            best_loss = val_loss\n","            torch.save(best_model.state_dict(), os.path.join(os.getcwd(), \"Modèles entraînés\", \"BILSTM_\" + model_name))\n","        else:\n","            p += 1\n","\n","        print(\"Train loss: {} - Train accuracy: {} -  Val loss: {} - Val accuracy: {}\".format(train_loss, train_accuracy\n","                                                                                              , val_loss, val_accuracy))\n","\n","        scheduler.step(val_loss)\n","\n","    return best_model, train_losses, train_accuracies, val_accuracies, val_losses"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TWko2GHMenNQ","colab_type":"code","colab":{}},"source":["def test(model, test_dataloader, show_results=False):  # TODO sortir ouput pour une chanson avec index chanson\n","    \"\"\"\n","    Test model on test songs\n","    :param mode: type of vector/feature to keep\n","    :param batch_size:\n","    :param model:\n","    :return:\n","    \"\"\"\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.to(device)\n","    model.eval()\n","    test_acc = 0\n","    nb_tr_steps = 0\n","    with torch.no_grad():\n","        for step, batch in enumerate(test_dataloader):\n","            # Add batch to GPU\n","            b_inputs, b_targets = tuple(t.to(device) for t in batch)\n","            outputs = model(b_inputs)\n","\n","            outputs = outputs.detach().cpu().numpy()\n","            b_targets = b_targets.to('cpu').numpy()\n","            \n","            if show_results:\n","              chords = np.argmax(outputs, 1)\n","              print([chord_dictionary[chord] for chord in chords])\n","\n","            test_acc += accuracy(outputs, b_targets)\n","            nb_tr_steps += 1\n","        test_accuracy = test_acc / nb_tr_steps\n","    print(\"Test accuracy: {}\".format(test_accuracy))\n","    pass\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KRYQYgggpUvl","colab_type":"code","colab":{}},"source":["def show_result(train_losses, train_accuracies, val_accuracies, val_losses):\n","    plt.plot(np.arange(len(val_accuracies)), train_losses, label='train loss')\n","    plt.plot(np.arange(len(val_accuracies)), val_losses, label='val loss')\n","    plt.legend(loc=\"upper right\")\n","    plt.title(\"Negative log-likelihood val-train\")\n","    plt.xlabel(\"Epochs\")\n","    plt.ylabel(\"Loss\")\n","    plt.show()\n","\n","    plt.plot(np.arange(len(val_accuracies)), val_accuracies, label='val accuracy')\n","    plt.plot(np.arange(len(val_accuracies)), train_accuracies, label='train accuracy')\n","    plt.legend()\n","    plt.title(\"Accuracy over validation/train-set\")\n","    plt.xlabel(\"Epochs\")\n","    plt.ylabel(\"Accuracy\")\n","    plt.show()\n","    pass"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jy6-gYT3IzrL","colab_type":"code","colab":{}},"source":["  train_dataloader, validation_dataloader, test_dataloader = data(batch_size=2048)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9swfTlIffAn3","colab_type":"code","outputId":"f76887ba-02c6-4b8e-8057-b9f3fee9f0fe","executionInfo":{"status":"ok","timestamp":1587337984757,"user_tz":240,"elapsed":1086580,"user":{"displayName":"M G","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiCpAKw7tbqA0zqKy0wtu5Q9vlxLL8FZofHR2NDseg=s64","userId":"10818001129093414844"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["  model = BiLSTM()\n","  best_model, train_losses, train_accuracies, val_accuracies, val_losses = train(model, train_dataloader,\n","                                                                                   validation_dataloader,\n","                                                                                   model_name=\"NAME\",\n","                                                                                   learning_rate=0.001)\n","print(\"DONE\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch:   0%|          | 0/400 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:50: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","Epoch:   0%|          | 1/400 [00:10<1:11:23, 10.73s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 2.595307633280754 - Train accuracy: 0.2709981944304912 -  Val loss: 2.2558361291885376 - Val accuracy: 0.3065909264984403\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:   0%|          | 2/400 [00:21<1:11:08, 10.73s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 2.2373235523700714 - Train accuracy: 0.29927323842012576 -  Val loss: 2.222332000732422 - Val accuracy: 0.3065909264984403\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:   1%|          | 3/400 [00:32<1:10:59, 10.73s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 2.144965708255768 - Train accuracy: 0.34442094811513096 -  Val loss: 2.0719350576400757 - Val accuracy: 0.386859011182598\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:   1%|          | 4/400 [00:42<1:10:38, 10.70s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 2.024556413292885 - Train accuracy: 0.40375434540764465 -  Val loss: 2.009894013404846 - Val accuracy: 0.4099297780191622\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:   1%|▏         | 5/400 [00:53<1:10:42, 10.74s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.9751720651984215 - Train accuracy: 0.4167338268839826 -  Val loss: 1.9563324451446533 - Val accuracy: 0.42215008251169783\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:   2%|▏         | 6/400 [01:04<1:10:45, 10.77s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.9354037567973137 - Train accuracy: 0.42498832390997304 -  Val loss: 1.9277170300483704 - Val accuracy: 0.4288986777239305\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:   2%|▏         | 7/400 [01:15<1:10:49, 10.81s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.9054341092705727 - Train accuracy: 0.43030138347585334 -  Val loss: 1.894133746623993 - Val accuracy: 0.43349648541945185\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:   2%|▏         | 8/400 [01:26<1:10:42, 10.82s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.8661317974328995 - Train accuracy: 0.43563621163187877 -  Val loss: 1.8778228163719177 - Val accuracy: 0.4289819292418672\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:   2%|▏         | 9/400 [01:37<1:10:36, 10.84s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.8349403887987137 - Train accuracy: 0.44200986091740735 -  Val loss: 1.835505723953247 - Val accuracy: 0.4445640196774733\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:   2%|▎         | 10/400 [01:47<1:10:29, 10.85s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.8125199377536774 - Train accuracy: 0.4469632193468703 -  Val loss: 1.813869833946228 - Val accuracy: 0.45254097732843135\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:   3%|▎         | 11/400 [01:58<1:10:31, 10.88s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.7966476380825043 - Train accuracy: 0.44925880143573893 -  Val loss: 1.7928386926651 - Val accuracy: 0.460144064728164\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:   3%|▎         | 12/400 [02:09<1:10:20, 10.88s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.7829324528574944 - Train accuracy: 0.4515408560656203 -  Val loss: 1.7960081696510315 - Val accuracy: 0.45219417578821297\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:   3%|▎         | 13/400 [02:20<1:10:30, 10.93s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.7761991024017334 - Train accuracy: 0.4533233873422135 -  Val loss: 1.7855868935585022 - Val accuracy: 0.4588768834948752\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:   4%|▎         | 14/400 [02:31<1:10:35, 10.97s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.7652896791696548 - Train accuracy: 0.4565214771726669 -  Val loss: 1.783898413181305 - Val accuracy: 0.4602467256433823\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:   4%|▍         | 15/400 [02:43<1:10:36, 11.00s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.759568341076374 - Train accuracy: 0.4561400074461044 -  Val loss: 1.7732197046279907 - Val accuracy: 0.46410445215017826\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:   4%|▍         | 16/400 [02:54<1:10:36, 11.03s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.7467928677797318 - Train accuracy: 0.4608794889002813 -  Val loss: 1.761108636856079 - Val accuracy: 0.4597313756823752\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:   4%|▍         | 17/400 [03:05<1:10:28, 11.04s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.7351641207933426 - Train accuracy: 0.466176273963455 -  Val loss: 1.7530149817466736 - Val accuracy: 0.46569136621267826\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:   4%|▍         | 18/400 [03:16<1:10:25, 11.06s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.7364671379327774 - Train accuracy: 0.4620952057946648 -  Val loss: 1.750471830368042 - Val accuracy: 0.4674294646836007\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:   5%|▍         | 19/400 [03:27<1:10:16, 11.07s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.722707062959671 - Train accuracy: 0.4669484702613819 -  Val loss: 1.7464251518249512 - Val accuracy: 0.46900462866254455\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:   5%|▌         | 20/400 [03:38<1:10:13, 11.09s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.7185357958078384 - Train accuracy: 0.4685514048981775 -  Val loss: 1.7328514456748962 - Val accuracy: 0.4719322707776292\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:   5%|▌         | 21/400 [03:49<1:10:08, 11.11s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.7088615596294403 - Train accuracy: 0.47099830523561603 -  Val loss: 1.7389689087867737 - Val accuracy: 0.46647997089460785\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:   6%|▌         | 22/400 [04:00<1:10:04, 11.12s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.70743977278471 - Train accuracy: 0.4700039908312453 -  Val loss: 1.7316952347755432 - Val accuracy: 0.46760771780303034\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:   6%|▌         | 23/400 [04:11<1:09:57, 11.14s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.7011348232626915 - Train accuracy: 0.4729604331520069 -  Val loss: 1.7261406779289246 - Val accuracy: 0.47107011927640374\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:   6%|▌         | 24/400 [04:23<1:09:47, 11.14s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.6941364109516144 - Train accuracy: 0.4754075874178565 -  Val loss: 1.7204376459121704 - Val accuracy: 0.47627827853442517\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:   6%|▋         | 25/400 [04:34<1:09:34, 11.13s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.6909957230091095 - Train accuracy: 0.4763609039260472 -  Val loss: 1.7258580923080444 - Val accuracy: 0.4683457536068405\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:   6%|▋         | 26/400 [04:45<1:09:18, 11.12s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.6810037195682526 - Train accuracy: 0.4799757133633935 -  Val loss: 1.7162086367607117 - Val accuracy: 0.4754064223345588\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:   7%|▋         | 27/400 [04:56<1:09:00, 11.10s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.6736074313521385 - Train accuracy: 0.4804064682860959 -  Val loss: 1.7073307037353516 - Val accuracy: 0.47929887686608735\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:   7%|▋         | 28/400 [05:07<1:08:51, 11.10s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.672094278037548 - Train accuracy: 0.4808529898546828 -  Val loss: 1.7071488499641418 - Val accuracy: 0.47713072533700984\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:   7%|▋         | 29/400 [05:18<1:08:47, 11.13s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.663835845887661 - Train accuracy: 0.4836811059902728 -  Val loss: 1.7065064907073975 - Val accuracy: 0.47945976597315065\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:   8%|▊         | 30/400 [05:29<1:08:35, 11.12s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.6667042449116707 - Train accuracy: 0.48262275545752176 -  Val loss: 1.7088878750801086 - Val accuracy: 0.4772819097454323\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:   8%|▊         | 31/400 [05:40<1:08:25, 11.13s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.654400773346424 - Train accuracy: 0.4851029204168046 -  Val loss: 1.699950098991394 - Val accuracy: 0.4760903207163547\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:   8%|▊         | 32/400 [05:52<1:08:19, 11.14s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.651669979095459 - Train accuracy: 0.4893108373691022 -  Val loss: 1.6974523067474365 - Val accuracy: 0.48040721437722816\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:   8%|▊         | 33/400 [06:03<1:08:09, 11.14s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.648484744131565 - Train accuracy: 0.48913800445879824 -  Val loss: 1.7000983953475952 - Val accuracy: 0.4823409299799465\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:   8%|▊         | 34/400 [06:14<1:07:57, 11.14s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.6473793759942055 - Train accuracy: 0.48685869687263617 -  Val loss: 1.694824993610382 - Val accuracy: 0.4820697206439394\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:   9%|▉         | 35/400 [06:25<1:07:47, 11.14s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.6410395801067352 - Train accuracy: 0.4897703785398544 -  Val loss: 1.6915540099143982 - Val accuracy: 0.4831954221674465\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:   9%|▉         | 36/400 [06:36<1:07:35, 11.14s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.6301572695374489 - Train accuracy: 0.4909085318468703 -  Val loss: 1.6894751191139221 - Val accuracy: 0.48399373154801695\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:   9%|▉         | 37/400 [06:47<1:07:21, 11.13s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.6251490116119385 - Train accuracy: 0.493947270057205 -  Val loss: 1.6886405944824219 - Val accuracy: 0.4817132144050802\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  10%|▉         | 38/400 [06:58<1:07:09, 11.13s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.6166332364082336 - Train accuracy: 0.4963061264892209 -  Val loss: 1.6760753989219666 - Val accuracy: 0.48340074399788324\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  10%|▉         | 39/400 [07:10<1:06:56, 11.13s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.6148341298103333 - Train accuracy: 0.4983815757301319 -  Val loss: 1.681517779827118 - Val accuracy: 0.4883938767268271\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  10%|█         | 40/400 [07:21<1:06:38, 11.11s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.6018415093421936 - Train accuracy: 0.5017034672031605 -  Val loss: 1.6764110922813416 - Val accuracy: 0.4885915392435383\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  10%|█         | 41/400 [07:32<1:06:27, 11.11s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.5965900719165802 - Train accuracy: 0.501698734900955 -  Val loss: 1.6787596344947815 - Val accuracy: 0.48953132833389035\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  10%|█         | 42/400 [07:43<1:06:24, 11.13s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.5943793430924416 - Train accuracy: 0.502757085433706 -  Val loss: 1.6757064461708069 - Val accuracy: 0.4906743938697638\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  11%|█         | 43/400 [07:54<1:06:11, 11.12s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.5877783447504044 - Train accuracy: 0.5041731518443883 -  Val loss: 1.6856607794761658 - Val accuracy: 0.476410053545566\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  11%|█         | 44/400 [08:05<1:06:03, 11.13s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.586657576262951 - Train accuracy: 0.505259018983134 -  Val loss: 1.6727095246315002 - Val accuracy: 0.4917459579712567\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  11%|█▏        | 45/400 [08:16<1:06:01, 11.16s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.5800344347953796 - Train accuracy: 0.5076036323397315 -  Val loss: 1.6670770049095154 - Val accuracy: 0.492825181386475\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  12%|█▏        | 46/400 [08:28<1:05:55, 11.17s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.566147543489933 - Train accuracy: 0.5118785863925397 -  Val loss: 1.6761858463287354 - Val accuracy: 0.4942782750529189\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  12%|█▏        | 47/400 [08:39<1:05:52, 11.20s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.5629134103655815 - Train accuracy: 0.5119388597635567 -  Val loss: 1.6669870615005493 - Val accuracy: 0.4959698954155526\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  12%|█▏        | 48/400 [08:50<1:05:38, 11.19s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.5513524636626244 - Train accuracy: 0.5150401567006075 -  Val loss: 1.6678128838539124 - Val accuracy: 0.4966926125919118\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  12%|█▏        | 49/400 [09:01<1:05:13, 11.15s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.5488434359431267 - Train accuracy: 0.5158671322821128 -  Val loss: 1.6637269854545593 - Val accuracy: 0.4953789532503342\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  12%|█▎        | 50/400 [09:12<1:05:07, 11.17s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.5415305644273758 - Train accuracy: 0.5177257035756075 -  Val loss: 1.6676734685897827 - Val accuracy: 0.4956307531890597\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  13%|█▎        | 51/400 [09:23<1:04:55, 11.16s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.5368621423840523 - Train accuracy: 0.5203559633102071 -  Val loss: 1.6603546738624573 - Val accuracy: 0.493838517296123\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  13%|█▎        | 52/400 [09:35<1:04:43, 11.16s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.5253107398748398 - Train accuracy: 0.5228799193781912 -  Val loss: 1.6560458540916443 - Val accuracy: 0.5021913905832219\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  13%|█▎        | 53/400 [09:46<1:04:34, 11.17s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.5171462297439575 - Train accuracy: 0.524738744600097 -  Val loss: 1.6606935262680054 - Val accuracy: 0.4961093297404189\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  14%|█▎        | 54/400 [09:57<1:04:23, 11.17s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.509623609483242 - Train accuracy: 0.5270503010944592 -  Val loss: 1.6612228751182556 - Val accuracy: 0.4976691750919118\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  14%|█▍        | 55/400 [10:08<1:04:19, 11.19s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.510588951408863 - Train accuracy: 0.5286199711094105 -  Val loss: 1.654948115348816 - Val accuracy: 0.4956522079712567\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  14%|█▍        | 56/400 [10:19<1:03:59, 11.16s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.5015548542141914 - Train accuracy: 0.5305183168258321 -  Val loss: 1.650083601474762 - Val accuracy: 0.5019007718499331\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  14%|█▍        | 57/400 [10:30<1:03:43, 11.15s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.4848091006278992 - Train accuracy: 0.5350066399971043 -  Val loss: 1.6596862077713013 - Val accuracy: 0.5003061984737076\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  14%|█▍        | 58/400 [10:41<1:03:26, 11.13s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.4842096790671349 - Train accuracy: 0.5357790902234423 -  Val loss: 1.6550534963607788 - Val accuracy: 0.5029064484458556\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  15%|█▍        | 59/400 [10:53<1:03:12, 11.12s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.4721228182315826 - Train accuracy: 0.5403276867657314 -  Val loss: 1.6552475690841675 - Val accuracy: 0.4966752485795455\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  15%|█▌        | 60/400 [11:04<1:03:01, 11.12s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.4707898497581482 - Train accuracy: 0.5409270501533543 -  Val loss: 1.6515865921974182 - Val accuracy: 0.501443650080771\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  15%|█▌        | 61/400 [11:15<1:02:56, 11.14s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.4645950645208359 - Train accuracy: 0.5413640609487282 -  Val loss: 1.6455554366111755 - Val accuracy: 0.5037880093443627\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  16%|█▌        | 62/400 [11:26<1:02:44, 11.14s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.4492096081376076 - Train accuracy: 0.5463401575131784 -  Val loss: 1.6478784680366516 - Val accuracy: 0.5043033593053698\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  16%|█▌        | 63/400 [11:37<1:02:33, 11.14s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.4406726732850075 - Train accuracy: 0.5497823879686318 -  Val loss: 1.6538132429122925 - Val accuracy: 0.5045669093276515\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  16%|█▌        | 64/400 [11:48<1:02:25, 11.15s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.4471757486462593 - Train accuracy: 0.5463344094973288 -  Val loss: 1.647552728652954 - Val accuracy: 0.5034703219000669\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  16%|█▋        | 65/400 [11:59<1:02:12, 11.14s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.4387956857681274 - Train accuracy: 0.5493751791349518 -  Val loss: 1.6413588523864746 - Val accuracy: 0.5099747764872995\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  16%|█▋        | 66/400 [12:11<1:01:56, 11.13s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.4252511709928513 - Train accuracy: 0.5552998137365852 -  Val loss: 1.641913115978241 - Val accuracy: 0.510189803016377\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  17%|█▋        | 67/400 [12:22<1:01:40, 11.11s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.4190322235226631 - Train accuracy: 0.5546144378708278 -  Val loss: 1.6495087146759033 - Val accuracy: 0.5073551171178698\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  17%|█▋        | 68/400 [12:33<1:01:29, 11.11s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.4112935662269592 - Train accuracy: 0.5575218489238606 -  Val loss: 1.6535608172416687 - Val accuracy: 0.5046327968332219\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  17%|█▋        | 69/400 [12:44<1:01:16, 11.11s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.4093885570764542 - Train accuracy: 0.5553773773239528 -  Val loss: 1.6496325731277466 - Val accuracy: 0.5043248140875669\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  18%|█▊        | 70/400 [12:55<1:00:59, 11.09s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.3980468660593033 - Train accuracy: 0.5615139036423861 -  Val loss: 1.6456494331359863 - Val accuracy: 0.5078628077651515\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  18%|█▊        | 71/400 [13:06<1:00:44, 11.08s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.3864665254950523 - Train accuracy: 0.5637404172034559 -  Val loss: 1.6435531377792358 - Val accuracy: 0.5048886875417781\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  18%|█▊        | 72/400 [13:17<1:00:35, 11.08s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.3702191188931465 - Train accuracy: 0.5703642552270471 -  Val loss: 1.6307786703109741 - Val accuracy: 0.5111760702150179\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  18%|█▊        | 73/400 [13:28<1:00:29, 11.10s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.341582641005516 - Train accuracy: 0.5791640519735132 -  Val loss: 1.6282318830490112 - Val accuracy: 0.5169016248189617\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  18%|█▊        | 74/400 [13:39<1:00:22, 11.11s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.3392794951796532 - Train accuracy: 0.5803369934728394 -  Val loss: 1.6271668076515198 - Val accuracy: 0.5183373544730392\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  19%|█▉        | 75/400 [13:50<1:00:13, 11.12s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.3336375281214714 - Train accuracy: 0.5805173518978701 -  Val loss: 1.6267818808555603 - Val accuracy: 0.5194553966828208\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  19%|█▉        | 76/400 [14:02<1:00:11, 11.15s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.325133040547371 - Train accuracy: 0.5844981183443055 -  Val loss: 1.6265510320663452 - Val accuracy: 0.5179537795231729\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  19%|█▉        | 77/400 [14:13<1:00:04, 11.16s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.3236133083701134 - Train accuracy: 0.5847305090091953 -  Val loss: 1.626482605934143 - Val accuracy: 0.5196801279105392\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  20%|█▉        | 78/400 [14:24<59:52, 11.16s/it]  "],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.3279768228530884 - Train accuracy: 0.5835415931043755 -  Val loss: 1.6255477666854858 - Val accuracy: 0.5186473826036097\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  20%|█▉        | 79/400 [14:35<59:40, 11.15s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.3244054168462753 - Train accuracy: 0.5841094232002293 -  Val loss: 1.6265704035758972 - Val accuracy: 0.5202051825701872\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  20%|██        | 80/400 [14:46<59:24, 11.14s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.321595475077629 - Train accuracy: 0.587239460216528 -  Val loss: 1.6269217133522034 - Val accuracy: 0.5191182998412434\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  20%|██        | 81/400 [14:57<59:15, 11.14s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.3149147182703018 - Train accuracy: 0.5885266925852165 -  Val loss: 1.6259176135063171 - Val accuracy: 0.5189109326258913\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  20%|██        | 82/400 [15:09<59:05, 11.15s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.3161443769931793 - Train accuracy: 0.5880204055024347 -  Val loss: 1.6247681975364685 - Val accuracy: 0.5200657482453208\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  21%|██        | 83/400 [15:20<58:56, 11.16s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.3107454031705856 - Train accuracy: 0.5887193088271795 -  Val loss: 1.6268839240074158 - Val accuracy: 0.5192965529606729\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  21%|██        | 84/400 [15:31<58:46, 11.16s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.3137956410646439 - Train accuracy: 0.5886070032163034 -  Val loss: 1.6276199221611023 - Val accuracy: 0.522570996615976\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  21%|██▏       | 85/400 [15:42<58:36, 11.16s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.3101962581276894 - Train accuracy: 0.5917828165997896 -  Val loss: 1.6274842619895935 - Val accuracy: 0.5207031685188279\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  22%|██▏       | 86/400 [15:53<58:29, 11.18s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.3063182160258293 - Train accuracy: 0.5915234171857271 -  Val loss: 1.6274287700653076 - Val accuracy: 0.5206663951091801\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  22%|██▏       | 87/400 [16:04<58:12, 11.16s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.304596833884716 - Train accuracy: 0.5896803586097059 -  Val loss: 1.6266238689422607 - Val accuracy: 0.5204687325924688\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  22%|██▏       | 88/400 [16:16<57:59, 11.15s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.3023349195718765 - Train accuracy: 0.5919961857182536 -  Val loss: 1.6259874105453491 - Val accuracy: 0.5205443247966801\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  22%|██▏       | 89/400 [16:27<57:55, 11.18s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.302104339003563 - Train accuracy: 0.5917953283451328 -  Val loss: 1.6258673071861267 - Val accuracy: 0.5215111825980392\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  22%|██▎       | 90/400 [16:38<57:56, 11.21s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.3033998236060143 - Train accuracy: 0.5924517102032314 -  Val loss: 1.6258686184883118 - Val accuracy: 0.5223462653882576\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  23%|██▎       | 91/400 [16:49<57:31, 11.17s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.304156631231308 - Train accuracy: 0.5904353108600842 -  Val loss: 1.6258550882339478 - Val accuracy: 0.5227686591326872\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  23%|██▎       | 92/400 [17:00<57:21, 11.17s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.3037626072764397 - Train accuracy: 0.5932026457678352 -  Val loss: 1.6253801584243774 - Val accuracy: 0.5221583075701872\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  23%|██▎       | 93/400 [17:11<57:03, 11.15s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.3006923347711563 - Train accuracy: 0.5932939445737991 -  Val loss: 1.625656545162201 - Val accuracy: 0.5214549997911097\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  24%|██▎       | 94/400 [17:23<56:52, 11.15s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.2968611866235733 - Train accuracy: 0.5929787501625142 -  Val loss: 1.6258982419967651 - Val accuracy: 0.5210326060466801\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  24%|██▍       | 95/400 [17:34<56:48, 11.17s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.3049456775188446 - Train accuracy: 0.5927203664620958 -  Val loss: 1.6258863806724548 - Val accuracy: 0.5211449716605392\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  24%|██▍       | 96/400 [17:45<56:39, 11.18s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.2970091551542282 - Train accuracy: 0.5941109015252695 -  Val loss: 1.6258520483970642 - Val accuracy: 0.5212670419730392\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  24%|██▍       | 97/400 [17:56<56:26, 11.18s/it]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 1.2994388565421104 - Train accuracy: 0.5927686359445915 -  Val loss: 1.6258712410926819 - Val accuracy: 0.5212670419730392\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-UbNiinWfhdv","colab_type":"code","colab":{}},"source":["  # Si la classe du modele change, on aura la nouvelle classe loadée\n","model_trained = BiLSTM()\n","model_trained.load_state_dict(torch.load(os.path.join(os.getcwd(), \"Modèles entraînés\", \"BILSTM.pth\")))\n","test(model_trained, test_dataloader, show_results=False)"],"execution_count":0,"outputs":[]}]}